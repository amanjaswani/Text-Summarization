# -*- coding: utf-8 -*-
"""Copy of Earning calls extractive summary bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RTotX5X24QLVGXTR4tCqsy8--WVa7gI9
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
print(os.listdir("../input"))

# Any results you write to the current directory are saved as output.

data = pd.read_csv("../input/Transcript_data.csv")
data = data[:150]
print (data.shape)
data.head()

# Import necessary libraries
import subprocess
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import nltk
import re
import string
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
stop_words = stopwords.words('english')

# install bert-as-service
!pip install bert-serving-server
!pip install bert-serving-client

# Download and unzip the pre-trained model
!wget http://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
!unzip uncased_L-12_H-768_A-12.zip

# Start the BERT server
bert_command = 'bert-serving-start -model_dir /kaggle/working/uncased_L-12_H-768_A-12'
process = subprocess.Popen(bert_command.split(), stdout=subprocess.PIPE)

# Start the BERT client
from bert_serving.client import BertClient
bc = BertClient(check_length=False)

# Function to tokenize sentences properly
def clean_tokenize(full_text):
    full_text = re.sub(r"\\'", "'", full_text)        # few modifications in text
    full_text = re.sub(r"U.S.", "US", full_text)
    #full_text = re.sub(r"[^a-zA-Z0-9]", " ", full_text)
    full_text = full_text.replace('\n','')
  
  
    tokenized = nltk.sent_tokenize(full_text)         # nltk tokenizer
  
    for sentence in tokenized:                        # identifying correct positions for tokenization
        x=re.findall(r'\w+[.?!][A-Z]+', sentence)
        all_delm = []
        punctuation = [".","!","?"]
        for punct in punctuation:
            for occurrence in x:
                try:
                    idx1 = occurrence.index(punct)
                    idx2 = sentence.index(occurrence)
                    punct_idx = idx1+idx2
                    all_delm .append(punct_idx)
                except:
                    continue
          
        all_delm.sort()
        good_tok = []
        lower_idx = 0
        higher_idx = 0
    
        for i in range(len(all_delm)):                  # creating list of properly tokenized text
            if i!=0:
                lower_idx = all_delm[i-1]+1
            higher_idx = all_delm[i]+1
            good_tok.append(sentence[lower_idx:higher_idx])
        good_tok.append(sentence[higher_idx:])
    
        sent_idx = tokenized.index(sentence)            
        for i in range(len(good_tok)):
            tokenized.insert(sent_idx+i+1, good_tok[i])
        tokenized.pop(sent_idx)
  
    #print (tokenized)
    return tokenized

# function to remove stopwords
def remove_stopwords(sen):
    sen_new = " ".join([i for i in sen if i not in stop_words])
    return sen_new

# convert sentence to vector
def convert_to_vectors(clean_sentences):
    sentence_vectors = []
    for i in clean_sentences:
            if i == '':
                sentence_vectors.append(bc.encode(["."]))
            else :
                sentence_vectors.append(bc.encode([i])) 
    return sentence_vectors

# calculate similarity of sentences
def calculate_similarities(clean_sentences,sentence_vectors):
    sim_mat = np.zeros([len(clean_sentences), len(clean_sentences)])           # create similarity matrix
    for i in range(len(clean_sentences)):
        for j in range(len(clean_sentences)):
            if i != j:
                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,768), sentence_vectors[j].reshape(1,768))[0,0]
    return sim_mat

# use textrank algorithm to calculate similarity
def TextRank(sim_mat):
    nx_graph = nx.from_numpy_array(sim_mat)
    scores = nx.pagerank(nx_graph)
    return scores

def generate_summary(text):
    sentences = clean_tokenize(text)                                                              # tokenize text
    sentences = list(filter(lambda a: a != "", sentences))                                        # remove empty string if any (Encode methods gives error if empty string is encountered)
    clean_sentences = pd.Series(sentences)                                                        # make a copy in pandas for processing
    clean_sentences = clean_sentences.apply(lambda s: s.translate(str.maketrans('', '', string.punctuation)))
    clean_sentences = [s.lower() for s in clean_sentences]                                        # make alphabets lowercase
    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]                      # remove stopwords from the sentences
    sentence_vectors = convert_to_vectors(clean_sentences)                                        # create vectors for sentences
    sim_mat = calculate_similarities(clean_sentences,sentence_vectors)                            # calculate similarities between sentences
    scores = TextRank(sim_mat)                                                                    # applying textrank algorithm
    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)      # sort sentences on the basis of similarity
    
    #-----Generating Summary-----#
    sn = 15                                                                                      # Specify number of sentences to form the summary
    summary = {}
    for i in range(sn):
        summary[sentences.index(ranked_sentences[i][1])] = ranked_sentences[i][1]
    
    counter=1
    final = ""
    for i in sorted (summary) : 
        final+= str(summary[i])+" "
        if (counter%5==0):
            final+="\n\n"
        counter+=1
    return final

for i in range(data.shape[0]):   #data.shape[0]
    text = data['Complete Transcript'][i]
    summ = generate_summary(text)
    data.at[i, 'Summary']=summ

data.to_csv("Earning_Calls_1.csv")